{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace link to LeonLLM\n",
    "- https://huggingface.co/collections/Leon-LLM/leon-llm-chess-6584387dbef870ffa4a7605f for all models and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to use private Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Login with your huggingface credentials\n",
    "Needs a private token to be generated from huggingface (https://huggingface.co/settings/tokens)\n",
    "Uses ipywidgets to display a login widget (!pip install ipywidgets)\n",
    "After installing ipywidgets, restart the kernel/program\n",
    "\"\"\"\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use a model from huggingface, saved in your local machine \n",
    "Default cache directory is ~/.cache/huggingface/hub\n",
    "You can change the cache directory by setting default_cache=False and inputting a cache_dir\n",
    "\"\"\"\n",
    "\n",
    "huggingface_repo_name = \"Leon-LLM/Leon-Chess-Mamba-350k-Plus-Right-Padding\"  # Input name of huggingface repo\n",
    "\n",
    "default_cache = True  # Set to False if you want to use a different cache directory\n",
    "cache_dir = \"./cache/huggingface/hub\"  # Input path to cache directory, will be ignored if default_cache=True\n",
    "\n",
    "if default_cache:\n",
    "    model = AutoModelForCausalLM.from_pretrained(huggingface_repo_name)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_repo_name, cache_dir=cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use a dataset from huggingface, saved in your local machine \n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset_name = (\n",
    "    \"Leon-LLM/Leon-Chess-Dataset-19k\"  # Input name of dataset from huggingface\n",
    ")\n",
    "\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"./data/test.tok\"  # Input path of text file to save dataset to\n",
    "\n",
    "\n",
    "def dataset_to_text_file(dataset, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in range(len(dataset)):\n",
    "            f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "\n",
    "\n",
    "data = dataset[\"train\"]\n",
    "dataset_to_text_file(data, text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Upload a model from your local machine (checkpoint_path) to huggingface\n",
    "Set private=True if you want to upload to a new private repo\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# checkpoint_path = \"/home/ubuntu/LeonLLM/leon-llm/Leon-LLM-Models/xLANplus/Leon-Chess_19k_0001_4E_PLUS/Leon-Chess_19k_0001_4E_PLUS\"  # Input name of local checkpoint\n",
    "# checkpoint_path = \"/home/ubuntu/LeonLLMV2/leon-llm/Leon-LLM-Models/V63_GPT2_350k_4E_xLANplus_RIGHT_PAD/checkpoint-70000\"  # Input name of local checkpoint\n",
    "checkpoint_path = \"./Leon-LLM-Models/R6_Mamba_71k_4E_xLANplus/R6_Mamba_71k_4E_xLANplus\"\n",
    "huggingface_repo_name = \"R6_Mamba_71k_4E_xLANplus\"  # Input name of huggingface repo\n",
    "\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.push_to_hub(huggingface_repo_name, organization=\"Leon-LLM\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_dir = (\n",
    "    \"Leon-LLM/V63_GPT2_350k_4E_xLANplus_RIGHT_PAD\"  # Hugging Face repo_id of base model\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "peft_model_dir = \"./Leon-LLM-Models/V66_LoRA_V63_GPT2-350k-Plus_98k_low_elo_4E_r64/V66_LoRA_V63_GPT2-350k-Plus_98k_low_elo_4E_r64\"\n",
    "model.load_adapter(peft_model_dir)\n",
    "\n",
    "peft_model = get_peft_model(model, model.peft_config[\"default\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_repo_name = (\n",
    "    \"V66_LoRA_V63_GPT2-350k-Plus_98k_low_elo_4E_r64\"  # Input name of huggingface repo\n",
    ")\n",
    "peft_model.push_to_hub(huggingface_repo_name, organization=\"Leon-LLM\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2829f462fa87407da3f921cf043ed335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bfb7a8782c4b6f97c88eb88f83bcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ed90f33239492ea2e2e9b21d3492e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2564b91305470aa5ef114ab8f6be57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfce8c487034b53ae5e381678b1d375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/271 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Upload a dataset from your local machine (checkpoint_path) to huggingface\n",
    "Set private=True if you want to upload to a new private repo\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset_name = \"Leon-LLM/270k_all_moves_xlan\"  # Input name of dataset for huggingface\n",
    "datset_path = \"C:/Users/Jerome/Coding/leon-llm/data/training/all_moves/270k_all_moves_noLong_BOS.tok\"  # Input path of dataset to upload\n",
    "my_dataset = Dataset.from_text(datset_path)\n",
    "\n",
    "my_dataset.push_to_hub(dataset_name, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
