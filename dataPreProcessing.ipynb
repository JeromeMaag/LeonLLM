{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PGN to extended LAN (xLAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing.pgn_to_xlan import pgn_to_xlan\n",
    "\n",
    "\n",
    "pgn_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03.pgn\"\n",
    "lan_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_2000plus.xlanplus\"\n",
    "\n",
    "min_number_of_moves_per_game = 0\n",
    "number_of_games_to_write = -1  # -1 for all games\n",
    "generate_all_moves = False\n",
    "\n",
    "pgn_to_lan = pgn_to_xlan(\n",
    "    pgn_path,\n",
    "    lan_path,\n",
    "    min_number_of_moves_per_game=min_number_of_moves_per_game,\n",
    "    number_of_games_to_write=number_of_games_to_write,\n",
    "    generate_all_moves=False,\n",
    "    log=False,\n",
    "    xLanPlus=True,\n",
    "    filter_elo=True,\n",
    "    elo_min=2000,\n",
    "    elo_max=3500,\n",
    ")\n",
    "\n",
    "pgn_to_lan.convert_pgn_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Common and Duplicate Lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use check_duplicates_and_common_lines to check if there are duplicates or common lines in two files.\n",
    "\"\"\"\n",
    "\n",
    "from src.data_preprocessing.check_duplicates_and_common_lines import (\n",
    "    check_duplicates_and_common_lines,\n",
    ")\n",
    "\n",
    "training_file = (\n",
    "    \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000.xlanplus\"\n",
    ")\n",
    "validation_file = (\n",
    "    \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000_old.xlanplus\"\n",
    ")\n",
    "\n",
    "check_duplicates_and_common_lines(\n",
    "    training_file,\n",
    "    validation_file,\n",
    "    delete_common=False,\n",
    "    delete_duplicates_from_file_1=True,\n",
    "    delete_duplicates_from_file_2=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer.tokenizer import tokenize_file\n",
    "\n",
    "notation = \"xLANplus\"  # select \"xLANplus\", \"xLAN\", \"xLANchk\" or \"xLANcap\"\n",
    "xLAN_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000.xlanplus\"\n",
    "tokenized_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000.tok\"\n",
    "\n",
    "tokenize_file(\n",
    "    notation=notation, data_path=xLAN_path, out_path=tokenized_path, batch_size=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer.detokenizer import detokenize_data\n",
    "\n",
    "notation = \"xLANplus\"  # select \"xLANplus\", \"xLAN\", \"xLANchk\" or \"xLANcap\"\n",
    "tokens = \"6 32 34 76 6 37 35 76 6 24 26 76 6 29 28 76 5 55 49 76 5 62 52 76 5 15 25 76 6 45 44 76 4 23 59 76 4 54 45 76 6 40 41 76 1 46 62 76 2 31 24 76 5 22 37 76 3 7 31 76 6 69 68 76 4 59 66 76 6 35 26 81 4 47 26 81 5 52 35 76 4 66 45 81 2 38 45 81 4 26 40 76 5 35 25 81 2 24 25 81 5 37 52 76 5 49 43 76 5 52 42 76 2 25 17 76 2 45 52 76 1 39 55 76 3 54 38 76 4 40 33 76 5 42 36 76 2 17 24 76 5 36 46 76 3 31 23 76 5 46 29 76 6 56 57 76 4 30 37 76 2 24 27 76 4 37 46 76 2 27 9 76 6 13 12 76 6 48 50 76 6 21 19 76 5 43 58 76 6 19 18 76 5 58 52 79 6 61 52 81 2 9 18 81 3 14 22 76 2 18 32 76 5 29 35 76 6 41 42 76 5 35 18 76 4 33 26 76 6 28 27 76 6 50 51 76 6 27 34 81 6 51 44 81 4 46 19 76 6 44 53 79 1 62 54 76 2 32 68 79 1 54 45 76 2 68 52 79 1 45 54 76 4 26 19 81 3 22 19 81 2 52 68 77 1 54 45 76 3 23 29 77 3 38 37 76 2 53 54 78 71 74\"\n",
    "\n",
    "print(detokenize_data(tokenized_data=tokens, notation=notation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove lines with more than x tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lines_with_too_many_tokens(\n",
    "    input_file_path, output_file_path, token_limit=510\n",
    "):\n",
    "    with open(input_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    print(f\"Number of lines in {input_file_path}: {len(lines)}\")\n",
    "    lines_to_keep = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if len(line.split()) <= token_limit:\n",
    "            lines_to_keep.append(line)\n",
    "        else:\n",
    "            removed_count += 1\n",
    "\n",
    "    print(f\"Number of lines in {output_file_path}: {len(lines_to_keep)}\")\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        file.writelines(lines_to_keep)\n",
    "\n",
    "    return removed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000.tok\"\n",
    "output_file_path = (\n",
    "    \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000_max_510.tok\"\n",
    ")\n",
    "removed_lines = remove_lines_with_too_many_tokens(input_file_path, output_file_path)\n",
    "print(f\"Number of removed lines: {removed_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create files with only one line for each starting sequence\n",
    "\n",
    "### Break down big Dataset to smaller dataset with more variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Remove Duplicate Lines by Start Sequence\n",
    "    -------------------\n",
    "\n",
    "    Removes duplicate lines from a file by comparing the first n tokens of each line.\n",
    "    The first n tokens are called the start sequence.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    lines: The lines to remove duplicates from.\n",
    "    start_sequences: A list of integers. Each integer is the length of the start sequence.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    A dictionary with the start sequence length as key and the list of lines as value.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remove_duplicates_by_start_sequence(lines, start_sequences, debug=True):\n",
    "    result = {length: [] for length in start_sequences}\n",
    "    starting_sequences_sets = {length: set() for length in start_sequences}\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        for length in start_sequences:\n",
    "            sequence = \" \".join(tokens[:length])\n",
    "            if sequence not in starting_sequences_sets[length]:\n",
    "                starting_sequences_sets[length].add(sequence)\n",
    "                result[length].append(line)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Original:\", len(lines))\n",
    "        for length in start_sequences:\n",
    "            print(\n",
    "                f\"Stripped duplicates for first {length} tokens:\", len(result[length])\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000_max_510.tok\"\n",
    ")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "start_sequences = [13, 16, 20, 24, 28, 32]\n",
    "\n",
    "\n",
    "results = remove_duplicates_by_start_sequence(lines, start_sequences, debug=True)\n",
    "\n",
    "\n",
    "for length, saved_lines in results.items():\n",
    "\n",
    "    out_path = f\"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03_elo_0_1000_max_510_{length}tokens.tok\"\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as file:\n",
    "\n",
    "        file.writelines(saved_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy File with BOS Token \"75 \" on start of every line into new file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"D:/LEON Safe/Datasets/2023_09/standard_rated_350k.xlanplus.tok\"\n",
    "out_path = \"D:/LEON Safe/Datasets/2023_09/standard_rated_350k.xlanplus.tok\"\n",
    "\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "saved_lines = []\n",
    "for line in lines:\n",
    "    new_line = \" 75 \" + line\n",
    "    new_line = new_line.replace(\"  \", \" \")\n",
    "    saved_lines.append(new_line)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(saved_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat Validation JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON file for Hard Position Accuracy \n",
    "### Make all moves for games in a Textfile and converts to a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import src.notation_converter as converter\n",
    "\n",
    "lan_file = \"./data/validation/hard_positions/hard_pos.xlanplus\"\n",
    "json_file = \"./data/validation/hard_positions/hard_positions_xlanplus.json\"\n",
    "games = []\n",
    "formatted_games = []\n",
    "c = 0\n",
    "xlanplus = True\n",
    "\n",
    "\n",
    "with open(lan_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if not line.strip() or line.startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    game = line[:-1]\n",
    "    formatted_game_start_square = {\n",
    "        \"id\": c,\n",
    "        \"board_state\": game,\n",
    "        \"legal_positions\": converter.legal_moves_from_position(game),\n",
    "    }\n",
    "    formatted_games.append(formatted_game_start_square)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "with open(json_file, \"w\") as file:\n",
    "    file.write(json.dumps(formatted_games, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Json File for legal piece move accuracy validation\n",
    "### Checks all positions of a Piece and saves it to a JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import src.notation_converter as converter\n",
    "\n",
    "lan_file = \"./data/validation/board_state/board_state_positions.xlanplus\"\n",
    "json_file = \"./data/validation/board_state/board_state_positions_xlanplus.json\"\n",
    "games = []\n",
    "formatted_games = []\n",
    "c = 0\n",
    "current_tag = None\n",
    "\n",
    "\n",
    "with open(lan_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "\n",
    "    if line.startswith(\"#\"):\n",
    "        # replace all hashtags with \"\"\n",
    "        current_tag = line.replace(\"#\", \"\").strip()\n",
    "        continue\n",
    "\n",
    "    game = line[:-1]\n",
    "    # Piece is last char of game\n",
    "    piece = game[-1]\n",
    "    board_state = game[:-1]\n",
    "    legal_start_positions = converter.positions_of_piece_with_legal_moves(\n",
    "        board_state, piece\n",
    "    )\n",
    "    formatted_game_start_square = {\n",
    "        \"id\": c,\n",
    "        \"board_state\": game,\n",
    "        \"legal_positions\": legal_start_positions,\n",
    "        \"piece\": piece,\n",
    "        \"tag\": current_tag,\n",
    "    }\n",
    "    first_start_position = legal_start_positions[0]\n",
    "    legal_end_positions = converter.end_positions_of_piece_with_legal_moves(\n",
    "        board_state, piece, first_start_position\n",
    "    )\n",
    "    formatted_game_end_square = {\n",
    "        \"id\": c + 1,\n",
    "        \"board_state\": game + first_start_position,\n",
    "        \"legal_positions\": legal_end_positions,\n",
    "        \"piece\": piece + first_start_position,\n",
    "        \"tag\": current_tag,\n",
    "    }\n",
    "    formatted_games.append(formatted_game_start_square)\n",
    "    formatted_games.append(formatted_game_end_square)\n",
    "\n",
    "    c += 2\n",
    "\n",
    "with open(json_file, \"w\") as file:\n",
    "    file.write(json.dumps(formatted_games, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Board of JSON ID from Hard Position Accuracy und Legal Piece Move Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Position Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.validate_position import get_position_by_id\n",
    "from src.validation.validate_position import show_position_by_id\n",
    "\n",
    "id_max = 66  # 0 to 66\n",
    "\n",
    "save_to_file = True\n",
    "\n",
    "for id in range(id_max + 1):\n",
    "    file_name = f\"hard_positions_{id:02}\"\n",
    "    print(f\"id = {id}\")\n",
    "    print(get_position_by_id(id, metric=\"hard_position\", notation=\"xLANplus\"))\n",
    "    show_position_by_id(\n",
    "        id, metric=\"hard_position\", notation=\"xLANplus\", save_path=file_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal Piece Move Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.validate_position import get_position_by_id\n",
    "from src.validation.validate_position import show_position_by_id\n",
    "\n",
    "id_max = 193  # 0 to 193\n",
    "\n",
    "for id in range(id_max + 1):\n",
    "    file_name = f\"legal_piece_moves_{id:02}\"\n",
    "    print(f\"id = {id}\")\n",
    "    print(get_position_by_id(id, metric=\"board_state\", notation=\"xLANplus\"))\n",
    "    show_position_by_id(\n",
    "        id, metric=\"board_state\", notation=\"xLANplus\", save_path=file_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert file from xLan to xLan+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notation_converter import xlan_sequence_to_xlanplus\n",
    "\n",
    "\n",
    "def convert_xlan_to_xlanplus(xlan_file, xlanplus_file):\n",
    "    with open(xlan_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    with open(xlanplus_file, \"w\") as file:\n",
    "        for line in lines:\n",
    "            # if empty line or line starts with # copy it\n",
    "            if not line.strip() or line.startswith(\"#\"):\n",
    "                file.write(line)\n",
    "                continue\n",
    "\n",
    "            xlan_plus = xlan_sequence_to_xlanplus(line)\n",
    "            file.write(xlan_plus + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlan_file = \"./data/validation/board_state/board_state_positions.lan\"\n",
    "xlanplus_file = \"./data/validation/board_state/board_state_positions.xlanplus\"\n",
    "convert_xlan_to_xlanplus(xlan_file, xlanplus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_path = \"D:/LEON Safe/Datasets/2024_03/standard_rated_2024-03.pgn\"\n",
    "output_path = \"D:/LEON Safe/Datasets/2024_03/elo_distribution.png\"\n",
    "\n",
    "\n",
    "def extract_elos(file_path):\n",
    "    number_of_elos = 0\n",
    "    elos = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if \"WhiteElo\" in line or \"BlackElo\" in line:\n",
    "                try:\n",
    "                    elo = int(line.split('\"')[1])\n",
    "                    elos.append(elo)\n",
    "                    number_of_elos += 1\n",
    "                    if number_of_elos % 100000 == 0:\n",
    "                        print(f\"Number of elos extracted: {number_of_elos}\")\n",
    "                except (IndexError, ValueError):\n",
    "                    pass  # Skip lines that don't have a valid number after WhiteElo or BlackElo\n",
    "    return elos\n",
    "\n",
    "\n",
    "elos = extract_elos(file_path)\n",
    "\n",
    "\n",
    "if elos:\n",
    "    # Calculate statistics\n",
    "    mean_elo = np.mean(elos)\n",
    "    median_elo = np.median(elos)\n",
    "    std_elo = np.std(elos)\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(elos, bins=range(0, 3000, 50), edgecolor=\"black\", alpha=0.7)\n",
    "    plt.title(\"Elo Distribution\", fontsize=15)\n",
    "    plt.xlabel(\"Elo\", fontsize=12)\n",
    "    plt.ylabel(\"Number of games\", fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Adding statistics as text on the plot\n",
    "    plt.text(\n",
    "        2000,\n",
    "        800,\n",
    "        f\"Mean: {mean_elo:.2f}\\nMedian: {median_elo:.2f}\\nStd Dev: {std_elo:.2f}\",\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "    # Adding error count as text on the plot\n",
    "    plt.text(\n",
    "        2000,\n",
    "        700,\n",
    "        f\"Errors: {errors}\",\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "    # Saving the plot\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid Elo data found in the PGN file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "median_elo = np.median(elos)\n",
    "average_elo = np.mean(elos)\n",
    "\n",
    "# Plot the Elo distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(elos, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "plt.axvline(1000, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "plt.axvline(2000, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "plt.axvline(\n",
    "    median_elo,\n",
    "    color=\"g\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Median Elo: {median_elo:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    average_elo,\n",
    "    color=\"b\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Average Elo: {average_elo:.2f}\",\n",
    ")\n",
    "plt.title(\"Elo Rating Distribution\")\n",
    "plt.xlabel(\"Elo Rating\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of Elo ratings under 1001, between 1001 and 1999, and over 1999\n",
    "under_1001 = sum(1 for elo in elos if elo <= 1000)\n",
    "between_1001_and_1999 = sum(1 for elo in elos if 1001 <= elo <= 1999)\n",
    "over_1999 = sum(1 for elo in elos if elo >= 2000)\n",
    "\n",
    "under_1001, between_1001_and_1999, over_1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_1001_and_1999 = sum(1 for elo in elos if 1500 <= elo <= 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_1001_and_1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
