{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a Chess LLM on xLAN Datasets/Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from src.train import ChessTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "- use datasets in ./data/training to train your model\n",
        "- use DownloadUpload.ipynb to Download a dataset from Hugging Face\n",
        "- use DataPreProcessing.ipynb to create your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CUDA\n",
        "Check if CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# xLAN\n",
        "\n",
        "ds_19k = \"./data/training/09_2023/xlan/standard_rated_19k.xlan.tok\"\n",
        "ds_71k = \"./data/training/09_2023/xlan/standard_rated_71k.xlan.tok\"\n",
        "ds_350k = \"./data/training/09_2023/xlan/standard_rated_350k.xlan.tok\"\n",
        "\n",
        "# xLAN+\n",
        "ds_19k_plus = \"./data/training/09_2023/xlanplus/standard_rated_19k.xlanplus.tok\"\n",
        "ds_71k_plus = \"./data/training/09_2023/xlanplus/standard_rated_71k.xlanplus.tok\"\n",
        "ds_350k_plus = \"./data/training/09_2023/xlanplus/standard_rated_350k.xlanplus.tok\"\n",
        "ds_1M_plus = \"./data/training/09_2023/standard_rated_1M.xlanplus.tok\"\n",
        "\n",
        "# finetune low elo\n",
        "ds_10k_low_elo = \"./data/training/03_2024/0_1000_elo/standard_rated_low_elo_10k_bos.tok\"\n",
        "ds_30k_low_elo = \"./data/training/03_2024/0_1000_elo/standard_rated_low_elo_30k_bos.tok\"\n",
        "ds_98k_low_elo = \"./data/training/03_2024/0_1000_elo/standard_rated_low_elo_98k_bos.tok\"\n",
        "\n",
        "# finetune high elo\n",
        "ds_10k_high_elo = (\n",
        "    \"./data/training/03_2024/2000_3500_elo/standard_rated_high_elo_10k_bos.tok\"\n",
        ")\n",
        "ds_30k_high_elo = (\n",
        "    \"./data/training/03_2024/2000_3500_elo/standard_rated_high_elo_30k_bos.tok\"\n",
        ")\n",
        "ds_98k_high_elo = (\n",
        "    \"./data/training/03_2024/2000_3500_elo/standard_rated_high_elo_98k_bos.tok\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = ds_30k_high_elo\n",
        "\n",
        "## HYPERPARAMETERS\n",
        "BATCH_SIZE = 16  # use the largest batch size that fits on your GPU\n",
        "SAVE_STEPS = 2000  # how often to save a checkpoint\n",
        "LOGGING_STEPS = 50  # how often to validate model and publish it to Weights & Biases\n",
        "EPOCHS = 4  # how many epochs to train for - how many times to go through the dataset\n",
        "LEARNING_RATE = 0.0001  # learning rate - how fast the model should learn\n",
        "SKIP_VALIDATION = False  # skip validation and only save model checkpoints\n",
        "WEIGHTS_AND_BIASES_ENABLED = True  # enable logging to Weights & Biases\n",
        "USE_FP16 = False  # enable mixed precision training (GPU only)\n",
        "XLANPLUS_ENABLED = True  # use xLanPlus tokenizer\n",
        "\n",
        "# PEFT_BASE_MODEL = \"Leon-LLM/Leon-Chess-350k-BOS\"  # base model to be loaded (from hugging face) for fine-tuning\n",
        "PEFT_BASE_MODEL = \"Leon-LLM/V63_GPT2_350k_4E_xLANplus_RIGHT_PAD\"  # base model to be loaded (from hugging face) for fine-tuning\n",
        "# PEFT_BASE_MODEL = \"./Leon-LLM-Models/V45_GPT2_19k_20E_xLANplus/checkpoint-24000\"  # base model to be loaded (from disk) for fine-tuning\n",
        "\n",
        "## NAMING\n",
        "MODEL_TYPE = \"GPT2\"  # choose between \"GPT2\" or \"Mamba\"\n",
        "VERSION_NUMBER = \"V69\"  # chronological version increment\n",
        "DATASET_SIZE = \"30k\"  # Options: 19k, 71k, 350k, 1M\n",
        "\n",
        "NOTATION = \"xLANplus\" if XLANPLUS_ENABLED else \"xLAN\"\n",
        "\n",
        "## CONFIG FOR FINE-TUNING\n",
        "R = 64  # 8~26min, 16~26min, 32~26min\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "peft_config = LoraConfig(  # https://huggingface.co/docs/peft/v0.10.0/en/package_reference/lora#peft.LoraConfig\n",
        "    task_type=\"CAUSAL_LM\",  # This does not need to be changed for our use case\n",
        "    inference_mode=False,  # don't change this for training, only later for inference\n",
        "    r=R,  # lower means faster training, but might underfit because of less complexity (experiments don't show that training time increases, which is rather weird)\n",
        "    lora_alpha=LORA_ALPHA,  # scaling factor that adjusts the magnitude of the combined result (balances the pretrained model’s knowledge and the new task-specific adaptation)\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    # use_rslora=True, # might work better (not tried yet)\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(\n",
        "    AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config\n",
        ")\n",
        "\n",
        "## MODEL NAME\n",
        "model_name = (\n",
        "    f\"{VERSION_NUMBER}_LoRA_V63_GPT2-350k-Plus_{DATASET_SIZE}_high_elo_{EPOCHS}E_r{R}\"\n",
        "    if PEFT_BASE_MODEL\n",
        "    else f\"{VERSION_NUMBER}_{MODEL_TYPE}_{DATASET_SIZE}_{EPOCHS}E_{NOTATION}\"\n",
        ")\n",
        "\n",
        "## SAVING MODEL\n",
        "output_dir = f\"./Leon-LLM-Models/{model_name}/\"\n",
        "\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = ChessTrainer(\n",
        "    model_type=MODEL_TYPE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    epochs=EPOCHS,\n",
        "    input_file=dataset,\n",
        "    output_dir=output_dir,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    skip_validation=SKIP_VALIDATION,\n",
        "    weight_and_biases=WEIGHTS_AND_BIASES_ENABLED,\n",
        "    use_FP16=USE_FP16,\n",
        "    notation=\"xLANplus\" if XLANPLUS_ENABLED else \"xLAN\",\n",
        "    peft=peft_model,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push Model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()\n",
        "peft_model.push_to_hub(model_name, organization=\"Leon-LLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model from Disk (fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/transformers/main/en/peft\n",
        "model_dir = \"./Leon-LLM-Models/V45_GPT2_19k_20E_xLANplus/checkpoint-24000\"\n",
        "peft_model_id = \"./Leon-LLM-Models/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128/V55_V45_GPT2_19k_20E_xLANplus_19k_1E_r128\"\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "loaded_model.load_adapter(peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "loaded_model.inference_mode = True\n",
        "loaded_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "loaded_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(\n",
        "    input=input,\n",
        "    num_tokens_to_generate=3,\n",
        "    model=loaded_model,\n",
        "    notation=\"xLAN\",\n",
        ")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference (for fine-tuned LoRA model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model from hugging face\n",
        "\n",
        "from src.generate_prediction import generate_prediction\n",
        "\n",
        "peft_model = get_peft_model(\n",
        "    AutoModelForCausalLM.from_pretrained(PEFT_BASE_MODEL), peft_config\n",
        ")\n",
        "peft_model.inference_mode = True\n",
        "peft_model.eval()\n",
        "input = \"Pd2d4 Pd7d5 Pc2c4 Pc7c6\"\n",
        "peft_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generate_prediction(\n",
        "    input=input,\n",
        "    num_tokens_to_generate=3,\n",
        "    model=peft_model,\n",
        "    notation=\"xLAN\",\n",
        ")[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
