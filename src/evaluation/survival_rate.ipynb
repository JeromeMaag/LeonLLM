{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SURVIVAL RATE AGAINST ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is designed to evaluate the performance of a GPT-2 model (or other architecture) trained on chess notation to compete against an automated chess engine. We assess the survival rate by simulating multiple games where the model and the engine face off under controlled conditions.\n",
    "\n",
    "## Key Objectives:\n",
    "\n",
    "1. **Model Loading**: Load a pre-trained chess model from Hugging Face.\n",
    "2. **Game Simulation**: Play a series of chess games between the model and the engine, recording game outcomes and move quality.\n",
    "3. **Performance Metrics**: Analyze the games to calculate the Average Centipawn Loss (ACPL) for each side, providing insights into the typical move quality and strategic depth exhibited by the model compared to the engine. Additionally, we calculate the survival rate of the model against the engine, i.e. the number of plies the model survives before losing the game. (It is expected that the engine will win most games.)\n",
    "4. **Statistical Analysis**: Compile and visualize the results to identify patterns and assess the model's consistency and robustness across games.\n",
    "5. **Integration with W&B**: Log the results and metrics to Weights & Biases for tracking experiments and further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from src.chess_game import ChessGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* models: https://huggingface.co/collections/Leon-LLM/leon-llm-chess-models-6584387dbef870ffa4a7605f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modelname = \"Leon-LLM/Leon-Chess-1M-BOS\"  # Huggingface model name\n",
    "# # modelname = \"Leon-LLM/Leon-Chess-350k-Plus\"\n",
    "# modelname = \"Leon-LLM/V63_GPT2_350k_4E_xLANplus_RIGHT_PAD\"\n",
    "# version = \"V63\"\n",
    "# model = GPT2LMHeadModel.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Leon-LLM/V63_GPT2_350k_4E_xLANplus_RIGHT_PAD\"\n",
    "# peft_model_id = \"Leon-LLM/V64_LoRA_V63_GPT2-350k-Plus_10k_low_elo_4E_r64\"\n",
    "# peft_model_id = \"Leon-LLM/V65_LoRA_V63_GPT2-350k-Plus_30k_low_elo_4E_r64\"\n",
    "# peft_model_id = \"Leon-LLM/V66_LoRA_V63_GPT2-350k-Plus_98k_low_elo_4E_r64\"\n",
    "# peft_model_id = \"Leon-LLM/V67_LoRA_V63_GPT2-350k-Plus_10k_high_elo_4E_r64\"\n",
    "# peft_model_id = \"Leon-LLM/V68_LoRA_V63_GPT2-350k-Plus_98k_high_elo_4E_r64\"\n",
    "peft_model_id = \"Leon-LLM/V69_LoRA_V63_GPT2-350k-Plus_30k_high_elo_4E_r64\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)\n",
    "version = peft_model_id.split(\"/\")[1].split(\"_\")[0]\n",
    "print(f\"Model version: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play One Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = \"model\"  # select \"player\", \"model\", or \"engine\" for white\n",
    "player2 = \"model\"  # select \"player\", \"model\", or \"engine\" for black\n",
    "notation = \"xLANplus\"  # select \"xLANplus\", \"xLAN\", \"xLANchk\" or \"xLANcap\"\n",
    "\n",
    "max_model_tries = 15\n",
    "temperature = (\n",
    "    0.1  # select a value between 0.1 and 2.0 to control the randomness of the model\n",
    ")\n",
    "show_game_history = False  # set to True to show the game history (takes up a lot of space) and keep the board for every move\n",
    "mate_score = 100_000\n",
    "\n",
    "game = ChessGame(\n",
    "    player1_type=player1,\n",
    "    player2_type=player2,\n",
    "    model_p1=model,\n",
    "    model_p2=model,\n",
    "    notation=notation,\n",
    "    max_model_tries=max_model_tries,\n",
    "    temperature=temperature,\n",
    "    starting_sequence=\"\",\n",
    "    show_game_history=show_game_history,\n",
    "    show_output=True,\n",
    "    manual_input=False,\n",
    "    mate_score=mate_score,\n",
    "    xlanplus=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.print_game_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "stats = (\n",
    "    game.get_stats()\n",
    ")  # self.movehistory, self.number_of_plies, self.outcome, self.board.result(), self.player_scores\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Average Centipawn Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acpl(scores_original, mate_score=100000):\n",
    "    # make deep copy of scores (to avoid modifying the original list)\n",
    "    scores = scores_original.copy()\n",
    "\n",
    "    # remove any score that is close to `mate_score=100000` (to avoid counting the mate score in the ACPL calculation)\n",
    "    threshold = 50  # threshold for removing scores close to mate score\n",
    "    scores = [score for score in scores if abs(score) < mate_score - threshold]\n",
    "\n",
    "    # Initialize variables\n",
    "    white_losses = 0\n",
    "    black_losses = 0\n",
    "    white_moves = 0\n",
    "    black_moves = 0\n",
    "\n",
    "    # Loop through the scores\n",
    "    for i in range(1, len(scores)):\n",
    "        previous_score = scores[i - 1]\n",
    "        current_score = scores[i]\n",
    "\n",
    "        # Calculate the score difference\n",
    "        score_diff = current_score - previous_score\n",
    "\n",
    "        if i % 2 == 1:  # White's turn (1-based index is odd)\n",
    "            white_moves += 1\n",
    "            if score_diff < 0:  # Loss for White\n",
    "                white_losses -= score_diff  # Subtract to make the loss positive\n",
    "        else:  # Black's turn (1-based index is even)\n",
    "            black_moves += 1\n",
    "            if score_diff > 0:  # Loss for Black\n",
    "                black_losses += score_diff\n",
    "\n",
    "    # Calculate ACPL\n",
    "    average_white_loss = white_losses / white_moves if white_moves else 0\n",
    "    average_black_loss = black_losses / black_moves if black_moves else 0\n",
    "\n",
    "    return average_white_loss, average_black_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = stats[4]\n",
    "\n",
    "acl_white, acl_black = calculate_acpl(scores)\n",
    "\n",
    "print(f\"Average Centipawn Loss for {player1} (White) = {acl_white}\")\n",
    "print(f\"Average Centipawn Loss for {player2} (Black) = {acl_black}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play many games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# play n games and collect stats\n",
    "n = 100\n",
    "stats = []\n",
    "\n",
    "# Config\n",
    "\n",
    "player1 = \"model\"  # select \"player\", \"model\", or \"engine\" for white\n",
    "player2 = \"model\"  # select \"player\", \"model\", or \"engine\" for black\n",
    "notation = \"xLANplus\"  # select \"xLANplus\", \"xLAN\", \"xLANchk\" or \"xLANcap\"\n",
    "\n",
    "max_model_tries = 15\n",
    "temperature = (\n",
    "    0.1  # select a value between 0.1 and 2.0 to control the randomness of the model\n",
    ")\n",
    "show_game_history = False  # set to True to show the game history (takes up a lot of space) and keep the board for every move\n",
    "mate_score = 100_000\n",
    "\n",
    "for i in range(n):\n",
    "    game = ChessGame(\n",
    "        player1_type=player1,\n",
    "        player2_type=player2,\n",
    "        model_p1=model,\n",
    "        model_p2=model,\n",
    "        notation=notation,\n",
    "        max_model_tries=max_model_tries,\n",
    "        temperature=temperature,\n",
    "        starting_sequence=\"\",\n",
    "        show_game_history=False,\n",
    "        show_output=False,\n",
    "        manual_input=False,\n",
    "        mate_score=mate_score,\n",
    "        xlanplus=True,\n",
    "    )\n",
    "\n",
    "    game.play_game()\n",
    "\n",
    "    stats.append(list(game.get_stats()))\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Game {i+1}/{n} finished ({game.outcome})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"results/survival_rate/survival_rate_temp01\"\n",
    "player1_name = f\"{version if player1 == 'model' else 'stockfish'}\"\n",
    "player2_name = f\"{version if player2 == 'model' else 'stockfish'}\"\n",
    "name = f\"{player1_name}_vs_{player2_name}_{n}_games\"\n",
    "print(base_path + \"/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_csv = pd.DataFrame(\n",
    "    stats,\n",
    "    columns=[\"Move History\", \"Number of Plies\", \"Outcome\", \"Result\", \"Player Scores\"],\n",
    ")\n",
    "\n",
    "# Calculate ACPL for each game\n",
    "acl_white_list = []\n",
    "acl_black_list = []\n",
    "\n",
    "for game_stats in stats:\n",
    "    scores = game_stats[4]\n",
    "    acl_white, acl_black = calculate_acpl(scores)\n",
    "    acl_white_list.append(acl_white)\n",
    "    acl_black_list.append(acl_black)\n",
    "    results_csv.loc[stats.index(game_stats), \"ACPL White\"] = acl_white\n",
    "    results_csv.loc[stats.index(game_stats), \"ACPL Black\"] = acl_black\n",
    "\n",
    "export_path_csv = f\"{base_path}/{name}_stats.csv\"\n",
    "results_csv.to_csv(export_path_csv, index=False)\n",
    "\n",
    "# Calculate average ACPL\n",
    "average_acl_white = sum(acl_white_list) / n\n",
    "average_acl_black = sum(acl_black_list) / n\n",
    "\n",
    "# Variance\n",
    "variance_acl_white = sum((acl - average_acl_white) ** 2 for acl in acl_white_list) / n\n",
    "variance_acl_black = sum((acl - average_acl_black) ** 2 for acl in acl_black_list) / n\n",
    "\n",
    "# Standard deviation\n",
    "std_dev_acl_white = variance_acl_white**0.5\n",
    "std_dev_acl_black = variance_acl_black**0.5\n",
    "\n",
    "# Max and min ACPL\n",
    "max_acl_white = max(acl_white_list)\n",
    "max_acl_black = max(acl_black_list)\n",
    "\n",
    "min_acl_white = min(acl_white_list)\n",
    "min_acl_black = min(acl_black_list)\n",
    "\n",
    "# Creating a pandas DataFrame to display the table\n",
    "\n",
    "df_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"Statistic\": [\n",
    "            \"Average Centipawn Loss\",\n",
    "            \"Variance of Centipawn Loss\",\n",
    "            \"Standard Deviation of Centipawn Loss\",\n",
    "            \"Maximum Centipawn Loss\",\n",
    "            \"Minimum Centipawn Loss\",\n",
    "        ],\n",
    "        f\"White ({player1_name})\": [\n",
    "            average_acl_white,\n",
    "            variance_acl_white,\n",
    "            std_dev_acl_white,\n",
    "            max_acl_white,\n",
    "            min_acl_white,\n",
    "        ],\n",
    "        f\"Black ({player2_name})\": [\n",
    "            average_acl_black,\n",
    "            variance_acl_black,\n",
    "            std_dev_acl_black,\n",
    "            max_acl_black,\n",
    "            min_acl_black,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Exporting the table to a CSV file\n",
    "export_path_table = f\"{base_path}/{name}_acpl_table.csv\"\n",
    "df_stats.to_csv(export_path_table, index=False)\n",
    "\n",
    "# Displaying the table\n",
    "display(df_stats)\n",
    "\n",
    "# Plot histogram of ACPL\n",
    "plt.hist(acl_white_list, bins=20, color=\"blue\", alpha=0.7, label=\"White\")\n",
    "plt.hist(acl_black_list, bins=20, color=\"red\", alpha=0.7, label=\"Black\")\n",
    "plt.xlabel(\"Average Centipawn Loss\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Average Centipawn Loss\")\n",
    "plt.legend()\n",
    "export_path_hist = f\"{base_path}/{name}_histogram.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(export_path_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot of results (only showing categories in which there are games)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "outcomes = [s[2] for s in stats]\n",
    "\n",
    "unique, counts = np.unique(outcomes, return_counts=True)\n",
    "\n",
    "plt.bar(unique, counts)\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.xticks(unique)\n",
    "plt.xticks(unique, rotation=45)\n",
    "plt.title(\"Outcome of games\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_path}/{name}_outcome.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of games won by checkmate:\", outcomes.count(\"checkmate\"))\n",
    "print(\"Number of games won by invalid_move:\", outcomes.count(\"invalid_move\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot of results (showing all categories)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "outcomes = [s[2] for s in stats]\n",
    "# possible results: 'invalid_move', 'checkmate', 'stalemate', 'insufficient_material', '75_move_rule', 'fivefold_repetition', 'unknown'\n",
    "# all of the results should be shown in the plot (even if they are not present in the data)\n",
    "\n",
    "unique_outcomes = [\n",
    "    \"invalid_move\",\n",
    "    \"checkmate\",\n",
    "    \"stalemate\",\n",
    "    \"insufficient_material\",\n",
    "    \"75_move_rule\",\n",
    "    \"fivefold_repetition\",\n",
    "    \"unknown\",\n",
    "]\n",
    "\n",
    "outcomes_dict = {outcome: outcomes.count(outcome) for outcome in unique_outcomes}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(outcomes_dict.keys(), outcomes_dict.values())\n",
    "ax.set_ylabel(\"Number of games\")\n",
    "ax.set_title(\"Outcome of Games\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_path}/{name}_outcome_all.png\")\n",
    "plt.show()\n",
    "\n",
    "# print numbers of games for each outcome\n",
    "for outcome, count in outcomes_dict.items():\n",
    "    print(f\"Number of games won by {outcome}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot of number of moves (ply) per game\n",
    "number_of_plies_until_end_of_game = [s[1] for s in stats]\n",
    "unique, counts = np.unique(number_of_plies_until_end_of_game, return_counts=True)\n",
    "\n",
    "plt.bar(unique, counts)\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.xlabel(\"Number of moves until end of game\")\n",
    "plt.title(\"Length of games\")\n",
    "plt.savefig(f\"{base_path}/{name}_length.png\")\n",
    "plt.show()\n",
    "\n",
    "# print average number of moves per game\n",
    "average_number_of_moves = sum(number_of_plies_until_end_of_game) / n\n",
    "print(f\"Average number of moves per game: {average_number_of_moves}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot of number of winners\n",
    "# all the possible winners will be displayed (even if they are not present in the data)\n",
    "\n",
    "results = [s[3] for s in stats]\n",
    "\n",
    "unique = [\"0-1\", \"1-0\", \"1/2-1/2\", \"*\"]\n",
    "counts = [results.count(u) for u in unique]\n",
    "\n",
    "plt.bar(unique, counts)\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.xlabel(\"Winner\")\n",
    "plt.title(\"Game Results\")\n",
    "plt.savefig(f\"{base_path}/{name}_results.png\")\n",
    "plt.show()\n",
    "\n",
    "# print numbers of games for each result\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Number of games won by {unique[i]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# # Guide for Resuming Runs: https://docs.wandb.ai/guides/runs/resuming\n",
    "\n",
    "# PROJECT_NAME = \"Leon LLM\"\n",
    "# RUN_ID = \"ee63i95h\" # The `run_id` of the model can be found in the 'Overview' panel of a run under 'Run Path'.\n",
    "\n",
    "# # Initialize W&B run\n",
    "# wandb.init(project=PROJECT_NAME, id=RUN_ID, resume=\"must\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## package the stats into a dictionary and check for correctness\n",
    "\n",
    "# # TODO: define the stats_dict dictionary\n",
    "# stats_dict = {\n",
    "#     \"average_acl_white\": average_acl_white,\n",
    "#     \"average_acl_black\": average_acl_black,\n",
    "#     \"variance_acl_white\": variance_acl_white,\n",
    "#     \"variance_acl_black\": variance_acl_black,\n",
    "#     \"std_dev_acl_white\": std_dev_acl_white,\n",
    "#     \"std_dev_acl_black\": std_dev_acl_black,\n",
    "#     \"max_acl_white\": max_acl_white,\n",
    "#     \"max_acl_black\": max_acl_black,\n",
    "#     \"min_acl_white\": min_acl_white,\n",
    "#     \"min_acl_black\": min_acl_black,\n",
    "#     \"number_of_games\": n,\n",
    "#     \"stats\": stats,\n",
    "# }\n",
    "# stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging new evaluation metrics\n",
    "# wandb.log(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of csv files in a directory\n",
    "paths = [\n",
    "    \"./results/survival_rate/survival_rate_temp01/V63\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V64\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V65\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V66\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V67\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V68\",\n",
    "    \"./results/survival_rate/survival_rate_temp01/V69\",\n",
    "]\n",
    "# recursively search for files ending in \"*acpl_table.csv\"\n",
    "csv_files = []\n",
    "for path in paths:\n",
    "    # print(f\"path = {path}\")\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # print(f\"file = {file}\")\n",
    "            if file.endswith(\"games_stats.csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "print(csv_files)\n",
    "\n",
    "# each of these files is a csv file containint a *column* called \"Outcome\"\n",
    "# extract the outcomes from each of these files\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # if csv_file contains \"V\" twice, skip it\n",
    "    if csv_file.count(\"V\") != 2:\n",
    "        continue\n",
    "    title = \"_\".join(csv_file.split(\"/\")[5].split(\"_\")[:3])\n",
    "    print(f\"title = {title}\")\n",
    "    with open(csv_file, \"r\") as f:\n",
    "        # put csv in pandas dataframe\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(f)\n",
    "        # extract the \"Outcome\" column\n",
    "        outcomes = df[\"Outcome\"]\n",
    "        # for every unique value in outcomes, count the number of times it appears\n",
    "        counts = outcomes.value_counts()\n",
    "        # calculate percentages\n",
    "        percentages = counts / counts.sum() * 100\n",
    "        # print the percentages\n",
    "        print(f\"percentages = {percentages}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
